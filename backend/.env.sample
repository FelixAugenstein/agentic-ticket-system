# Insert below "watsonx" or "ollama" to use either LLM from watsonx or local Ollama model
AI_PLATFORM="ollama"

OLLAMA_MODEL_ID="mistral-small3.1:latest"
OLLAMA_MODEL_ID_ALT_1="qwen3:latest"
OLLAMA_MODEL_ID_ALT_2="granite3.1-dense:latest"
OLLAMA_MODEL_ID_ALT_3="granite3.2:8b"
OLLAMA_MODEL_ID_ALT_4="mistral:7b"

WATSONX_URL="https://us-south.ml.cloud.ibm.com"
WATSONX_APIKEY="YOUR_API_KEY_HERE"
WATSONX_PROJECT_ID="YOUR_PROJECT_ID_HERE"
WATSONX_MODEL_ID="mistralai/mistral-large"